import os, json, time, re
from collections import defaultdict
from typing import Optional, List, Tuple

import feedparser
import requests
from dateutil import parser as dtparser
from openai import OpenAI


# ----------------- ENV / CLIENT -----------------

TELEGRAM_BOT_TOKEN = os.environ["TELEGRAM_BOT_TOKEN"]
TELEGRAM_CHANNEL = os.environ["TELEGRAM_CHANNEL"]

GROQ_API_KEY = os.environ.get("GROQ_API_KEY", "").strip()
if not GROQ_API_KEY:
    raise RuntimeError("GROQ_API_KEY is empty. Check GitHub Secrets.")

client = OpenAI(
    api_key=GROQ_API_KEY,
    base_url="https://api.groq.com/openai/v1",
)

GROQ_MODEL = os.environ.get("GROQ_MODEL", "llama-3.1-8b-instant")

TOTAL_LIMIT = int(os.environ.get("TOTAL_LIMIT", "15"))         # —Å–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤–∑—è—Ç—å –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã (–¥–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤)
PER_FEED_SCAN = int(os.environ.get("PER_FEED_SCAN", "20"))     # —Å–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π —á–∏—Ç–∞—Ç—å –∏–∑ –∫–∞–∂–¥–æ–≥–æ RSS
MAX_SUMMARY_CHARS = int(os.environ.get("MAX_SUMMARY_CHARS", "280"))

HOT_HOURS = int(os.environ.get("HOT_HOURS", "6"))

ARG_FILTER = os.environ.get("ARG_FILTER", "1") == "1"          # 1 = —Å—Ç—Ä–æ–≥–æ –ê—Ä–≥–µ–Ω—Ç–∏–Ω–∞

FEEDS_FILE = "feeds.json"
STATE_FILE = "state.json"

HTTP_TIMEOUT = int(os.environ.get("HTTP_TIMEOUT", "20"))       # —Ç–∞–π–º–∞—É—Ç –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü (–¥–ª—è og:image)

# –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —á–∏—Å–ª–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –ø–æ—Å—Ç–µ
MIN_NEWS = int(os.environ.get("MIN_NEWS", "2"))
MAX_NEWS = int(os.environ.get("MAX_NEWS", "6"))


# ----------------- RUBRICS -----------------

RUBRICS = {
    "üî• –ì–æ—Ä—è—á–µ–µ": [
        "urgente", "√∫ltimo momento", "ultima hora", "en vivo", "ahora", "breaking",
        "alerta", "se confirm√≥", "confirm√≥", "confirmaron"
    ],
    "üí∞ –≠–∫–æ–Ω–æ–º–∏–∫–∞": [
        "econom√≠a", "economia", "inflaci√≥n", "inflacion", "ipc", "√≠ndice", "indice", "indec",
        "recesi√≥n", "recesion", "d√≥lar", "dolar", "blue", "mep", "ccl", "reservas",
        "banco central", "bcr", "bcra", "fmi", "deuda", "bonos", "mercados",
        "riesgo pa√≠s", "riesgo pais", "tasas", "exportaciones", "importaciones",
        "subsidios", "tarifas", "salarios", "paritarias", "pymes", "impuestos",
        "retenciones", "cepo", "devaluaci√≥n", "devaluacion"
    ],
    "üèõ –ü–æ–ª–∏—Ç–∏–∫–∞": [
        "milei", "presidente", "gobierno", "gabinete", "casa rosada", "jefe de gabinete",
        "congreso", "senado", "diputados", "ley", "decreto", "dnu", "bolet√≠n oficial", "boletin oficial",
        "oposici√≥n", "oposicion", "peronismo", "kirchnerismo", "pro", "ucr",
        "kicillof", "massa", "bullrich", "macri", "larreta", "elecciones", "balotaje", "campa√±a"
    ],
    "üè¢ –ë–∏–∑–Ω–µ—Å / –∫–æ–º–ø–∞–Ω–∏–∏": [
        "empresa", "empresas", "negocio", "negocios", "inversi√≥n", "inversion",
        "startup", "fintech", "banco", "bancos", "mercado libre", "ypf",
        "telecom", "personal", "movistar", "claro", "aerol√≠neas", "aerolineas",
        "industria", "comercio"
    ],
    "‚öñÔ∏è –°—É–¥ / –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å": [
        "polic√≠a", "policia", "crimen", "delito", "robo", "homicidio", "asesinato",
        "detenido", "detuvieron", "allanamiento", "operativo", "narco", "drogas",
        "juez", "jueza", "fiscal", "tribunal", "causa", "condena", "juicio",
        "seguridad", "gendarmer√≠a", "gendarmeria", "prefectura"
    ],
    "üåé –û–±—â–µ—Å—Ç–≤–æ": [
        "salud", "hospital", "educaci√≥n", "educacion", "escuela", "universidad",
        "paro", "huelga", "sindicato", "cgt", "protesta", "marcha",
        "transporte", "subte", "colectivo", "tren", "vivienda", "alquiler",
        "inmuebles", "piquete", "servicios", "luz", "gas", "agua", "anmat"
    ],
    "üß™ –ù–∞—É–∫–∞ / —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": [
        "tecnolog√≠a", "tecnologia", "ia", "inteligencia artificial", "software",
        "ciber", "ciberseguridad", "datos", "internet", "sat√©lite", "satelite",
        "investigaci√≥n", "investigacion", "conicet"
    ],
    "üå¶ –ü–æ–≥–æ–¥–∞ / –ß–°": [
        "tormenta", "lluvia", "granizo", "ola de calor", "ola de frio",
        "inundaci√≥n", "inundacion", "alerta meteorol√≥gica", "alerta meteorologica",
        "evacuados", "incendio", "sismo"
    ],
    "üé≠ –ö—É–ª—å—Ç—É—Ä–∞": [
        "cultura", "cine", "teatro", "m√∫sica", "musica", "festival", "libro",
        "feria del libro", "arte", "exposici√≥n", "exposicion", "concierto"
    ],
    "‚öΩ –°–ø–æ—Ä—Ç": [
        "f√∫tbol", "futbol", "river", "boca", "selecci√≥n", "seleccion", "messi",
        "copa", "liga", "mundial", "afa", "racing", "independiente", "san lorenzo"
    ],
}

RUBRIC_ORDER = [
    "üî• –ì–æ—Ä—è—á–µ–µ",
    "üí∞ –≠–∫–æ–Ω–æ–º–∏–∫–∞",
    "üèõ –ü–æ–ª–∏—Ç–∏–∫–∞",
    "üè¢ –ë–∏–∑–Ω–µ—Å / –∫–æ–º–ø–∞–Ω–∏–∏",
    "‚öñÔ∏è –°—É–¥ / –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
    "üåé –û–±—â–µ—Å—Ç–≤–æ",
    "üß™ –ù–∞—É–∫–∞ / —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
    "üå¶ –ü–æ–≥–æ–¥–∞ / –ß–°",
    "üé≠ –ö—É–ª—å—Ç—É—Ä–∞",
    "‚öΩ –°–ø–æ—Ä—Ç",
]

ARG_HINTS = [
     # —Å—Ç—Ä–∞–Ω–∞ / –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ
    "argentina", "argentino", "argentina",

    # —Å—Ç–æ–ª–∏—Ü–∞ –∏ –∞–≥–ª–æ–º–µ—Ä–∞—Ü–∏—è
    "buenos aires", "caba", "amba", "gba",

    # –ø—Ä–æ–≤–∏–Ω—Ü–∏–∏/–≥–æ—Ä–æ–¥–∞ (—á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç—è—Ö)
    "c√≥rdoba", "cordoba", "rosario", "mendoza", "la plata",
    "santa fe", "tucum√°n", "tucuman", "salta", "neuqu√©n", "neuquen",
    "san juan", "san luis", "chaco", "misiones", "corrientes",
    "entre r√≠os", "entre rios", "r√≠o negro", "rio negro",
    "chubut", "santa cruz", "tierra del fuego", "ushuaia",
    "mar del plata", "bah√≠a blanca", "bahia blanca",

    # –ø–æ–ª–∏—Ç–∏–∫–∞ / –∏–Ω—Å—Ç–∏—Ç—É—Ç—ã
    "milei", "casa rosada", "gobierno", "presidente",
    "congreso", "senado", "diputados", "bolet√≠n oficial", "boletin oficial",

    # —ç–∫–æ–Ω–æ–º–∏–∫–∞ / —Ä–µ–≥—É–ª—è—Ç–æ—Ä—ã
    "indec", "banco central", "bcra", "afip", "anmat",

    # –∞—Ä–≥–µ–Ω—Ç–∏–Ω—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö
    "subte", "colectivo", "tren roca", "tren mitre", "tren sarmiento",
    "aerolineas argentinas", "ypf", "mercado libre", "edenor", "edesur",
]


# ----------------- JSON / TELEGRAM -----------------

def load_json(path, default):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return default


def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def tg_send_message(text: str):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    r = requests.post(
        url,
        json={
            "chat_id": TELEGRAM_CHANNEL,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        },
        timeout=30,
    )
    r.raise_for_status()


def tg_send_photo(photo_url: str, caption: str):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendPhoto"
    r = requests.post(
        url,
        data={
            "chat_id": TELEGRAM_CHANNEL,
            "photo": photo_url,
            "caption": caption,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        },
        timeout=30,
    )
    r.raise_for_status()


# ----------------- HELPERS -----------------

def html_escape(s: str) -> str:
    return (s or "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")


def clean_text(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s


def pick_time(entry) -> float:
    for key in ("published", "updated", "created"):
        if key in entry and entry[key]:
            try:
                return dtparser.parse(entry[key]).timestamp()
            except Exception:
                pass
    return time.time()


def is_argentina_related(title: str, summary: str, link: str = "") -> bool:
    if not ARG_FILTER:
        return True
    t = (title + " " + summary + " " + (link or "")).lower()
    return any(h in t for h in ARG_HINTS)


def is_hot(ts: float, title: str, summary: str) -> bool:
    # —Å–≤–µ–∂–µ–µ –∑–∞ HOT_HOURS —á–∞—Å–æ–≤ ‚Äî –≥–æ—Ä—è—á–µ–µ
    if (time.time() - ts) <= HOT_HOURS * 3600:
        return True
    t = (title + " " + summary).lower()
    return any(w in t for w in RUBRICS["üî• –ì–æ—Ä—è—á–µ–µ"])


def detect_rubric(ts: float, title: str, summary: str) -> str:
    if is_hot(ts, title, summary):
        return "üî• –ì–æ—Ä—è—á–µ–µ"
    t = (title + " " + summary).lower()
    for rubric in RUBRIC_ORDER:
        if rubric == "üî• –ì–æ—Ä—è—á–µ–µ":
            continue
        keys = RUBRICS.get(rubric, [])
        if any(k in t for k in keys):
            return rubric
    return "üåé –û–±—â–µ—Å—Ç–≤–æ"


# ----------------- IMAGE EXTRACTION (RSS -> HTML og:image) -----------------

UA = "Mozilla/5.0 (compatible; ArgentinaDigestBot/1.0; +https://github.com/)"

def extract_image_from_rss(entry) -> Optional[str]:
    if hasattr(entry, "media_content"):
        try:
            for m in entry.media_content:
                u = m.get("url")
                if u:
                    return u
        except Exception:
            pass

    if hasattr(entry, "links"):
        try:
            for l in entry.links:
                href = l.get("href")
                ltype = (l.get("type") or "").lower()
                rel = (l.get("rel") or "").lower()
                if href and (ltype.startswith("image/") or rel == "enclosure"):
                    return href
        except Exception:
            pass

    summary = getattr(entry, "summary", "") or getattr(entry, "description", "") or ""
    m = re.search(r'<img[^>]+src="([^"]+)"', summary)
    if m:
        return m.group(1)

    return None


def extract_og_image_from_html(url: str) -> Optional[str]:
    try:
        r = requests.get(
            url,
            headers={"User-Agent": UA},
            timeout=HTTP_TIMEOUT,
            allow_redirects=True,
        )
        if r.status_code >= 400:
            return None
        html = r.text
    except Exception:
        return None

    patterns = [
        r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']',
        r'<meta[^>]+content=["\']([^"\']+)["\'][^>]+property=["\']og:image["\']',
        r'<meta[^>]+name=["\']twitter:image["\'][^>]+content=["\']([^"\']+)["\']',
        r'<meta[^>]+content=["\']([^"\']+)["\'][^>]+name=["\']twitter:image["\']',
    ]
    for p in patterns:
        m = re.search(p, html, flags=re.IGNORECASE)
        if m:
            img = m.group(1).strip()
            if img.startswith("//"):
                img = "https:" + img
            return img

    return None


def best_image(entry, link: str) -> Optional[str]:
    img = extract_image_from_rss(entry)
    if img:
        return img
    return extract_og_image_from_html(link)


# ----------------- GROQ SUMMARIZER -----------------

def _call_groq_chat(messages, model: str, max_retries: int = 3):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.2,
                max_tokens=320,
            )
        except Exception as e:
            msg = str(e)
            if attempt < max_retries - 1 and ("429" in msg or "Rate limit" in msg or "timeout" in msg or "5" in msg):
                time.sleep(1.5 * (attempt + 1))
                continue
            raise


def summarize_to_ru(title: str, snippet: str) -> str:
    title = clean_text(title)
    snippet = clean_text(snippet)

    base = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n–¢–µ–∫—Å—Ç: {snippet}" if snippet else f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}"

    resp = _call_groq_chat(
        model=GROQ_MODEL,
        messages=[
            {
                "role": "system",
                "content": (
                    "–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –∏ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π. –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –Ω–∞ –∏—Å–ø–∞–Ω—Å–∫–æ–º (–ê—Ä–≥–µ–Ω—Ç–∏–Ω–∞). "
                    "–°–¥–µ–ª–∞–π –ø–æ–Ω—è—Ç–Ω—É—é –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—É—é –≤—ã–∂–∏–º–∫—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
                    "–°—Ç–∏–ª—å: –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–Ω—ã–π, –±–µ–∑ –æ—Ü–µ–Ω–∫–∏ –∏ –∫–ª–∏—à–µ. "
                    "–ü–∏—à–∏ –ø—Ä–æ—Å—Ç–æ, –∫–∞–∫ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –ª—é–¥–µ–π. "
                    "–î–ª–∏–Ω–∞: 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –ù–µ –¥–æ–±–∞–≤–ª—è–π —Ñ–∞–∫—Ç–æ–≤ –∏–ª–∏ –∏–¥–µ–π, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤–æ –≤—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ."
                ),
            },
            {"role": "user", "content": base},
        ],
    )

    text = (resp.choices[0].message.content or "").strip()
    text = clean_text(text)
    if len(text) > MAX_SUMMARY_CHARS:
        text = text[: MAX_SUMMARY_CHARS - 1].rstrip() + "‚Ä¶"
    return text


# ----------------- MAIN -----------------

Item = Tuple[float, str, str, str, str, Optional[str]]  # (ts, source, title, link, summary, image_url)

def main():
    feeds = load_json(FEEDS_FILE, [])
    state = load_json(STATE_FILE, {"seen_links": []})
    seen = set(state.get("seen_links", []))

    candidates: List[Item] = []

    for f in feeds:
        name, url = f["name"], f["url"]
        d = feedparser.parse(url)

        entries: List[Item] = []
        for e in d.entries[:PER_FEED_SCAN]:
            link = getattr(e, "link", None)
            title = getattr(e, "title", "").strip()
            summary = getattr(e, "summary", "") or getattr(e, "description", "") or ""

            if not link or not title:
                continue
            if link in seen:
                continue

            ts = pick_time(e)
            image_url = best_image(e, link)

            entries.append((ts, name, title, link, summary, image_url))

        entries.sort(key=lambda x: x[0], reverse=True)
        candidates.extend(entries)

    candidates.sort(key=lambda x: x[0], reverse=True)
    picked = candidates[:TOTAL_LIMIT]

    if not picked:
        tg_send_message("–°–µ–≥–æ–¥–Ω—è –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –Ω–µ –Ω–∞—à—ë–ª.")
        return

    # --- –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä—É–±—Ä–∏–∫–∞–º ---
    grouped = defaultdict(list)
    for ts, source, title, link, summary, image_url in picked:
        if not is_argentina_related(title, summary, link):
            continue
        rubric = detect_rubric(ts, title, summary)
        grouped[rubric].append((ts, source, title, link, summary, image_url))

    if not any(grouped.values()):
        tg_send_message("–°–µ–≥–æ–¥–Ω—è –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –Ω–µ –Ω–∞—à—ë–ª –Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É.")
        return

    # -------- –ê–î–ê–ü–¢–ò–í–ù–´–ô –û–¢–ë–û–† MIN_NEWS‚ÄìMAX_NEWS --------
    selected: List[Tuple[str, Item]] = []

    # 1) –≥–æ—Ä—è—á–∏–µ –ø–µ—Ä–≤—ã–º–∏
    hot_items = grouped.get("üî• –ì–æ—Ä—è—á–µ–µ", [])
    hot_items.sort(key=lambda x: x[0], reverse=True)
    for item in hot_items:
        if len(selected) >= MAX_NEWS:
            break
        selected.append(("üî• –ì–æ—Ä—è—á–µ–µ", item))

    # 2) –∑–∞—Ç–µ–º —Ä—É–±—Ä–∏–∫–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    for rubric in RUBRIC_ORDER:
        if rubric == "üî• –ì–æ—Ä—è—á–µ–µ":
            continue
        items = grouped.get(rubric, [])
        items.sort(key=lambda x: x[0], reverse=True)
        for item in items:
            if len(selected) >= MAX_NEWS:
                break
            selected.append((rubric, item))
        if len(selected) >= MAX_NEWS:
            break

    # 3) –¥–æ–±–∏—Ä–∞–µ–º –¥–æ MIN_NEWS –∏–∑ –æ–±—â–µ–≥–æ –ø—É–ª–∞, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –º–∞–ª–æ
    if len(selected) < MIN_NEWS:
        flat: List[Tuple[str, Item]] = []
        for r, items in grouped.items():
            for it in items:
                flat.append((r, it))
        flat.sort(key=lambda x: x[1][0], reverse=True)

        existing = set((r, it[3]) for r, it in selected)  # (rubric, link)
        for r, it in flat:
            key = (r, it[3])
            if key in existing:
                continue
            selected.append((r, it))
            existing.add(key)
            if len(selected) >= MIN_NEWS:
                break

    if not selected:
        tg_send_message("–°–µ–≥–æ–¥–Ω—è –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –Ω–µ –Ω–∞—à—ë–ª –Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É.")
        return

    # -------- –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –ü–û–°–¢–ê --------
    lines = ["<b>–ê—Ä–≥–µ–Ω—Ç–∏–Ω–∞ ‚Äî –µ–∂–µ–¥–Ω–µ–≤–Ω–∞—è –≤—ã–∂–∏–º–∫–∞</b>\n"]
    new_links: List[str] = []

    current_rubric = None
    for rubric, (ts, source, title, link, summary, image_url) in selected:
        if rubric != current_rubric:
            lines.append(f"<b>{html_escape(rubric)}</b>")
            current_rubric = rubric

        ru = summarize_to_ru(title, summary)
        lines.append(
            f"‚Ä¢ <a href=\"{html_escape(link)}\">{html_escape(title)}</a> "
            f"<i>({html_escape(source)})</i>"
        )
        if ru:
            lines.append(f"  {html_escape(ru)}")
        lines.append("")
        new_links.append(link)

    text = "\n".join(lines).strip()
    if len(text) > 3800:
        text = text[:3790] + "‚Ä¶"

    # ---- Variant B: one lead image (–±–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É —Å—Ä–µ–¥–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö) ----
    lead_image = None
    for rubric, (ts, source, title, link, summary, image_url) in selected:
        if image_url:
            lead_image = image_url
            break

    if lead_image:
        # caption limit ~1024, –æ—Å—Ç–∞–≤–∏–º –∑–∞–ø–∞—Å
        if len(text) <= 950:
            tg_send_photo(lead_image, text)
        else:
            short_caption = "<b>–ê—Ä–≥–µ–Ω—Ç–∏–Ω–∞ ‚Äî –µ–∂–µ–¥–Ω–µ–≤–Ω–∞—è –≤—ã–∂–∏–º–∫–∞</b>\n\n–°–≤–æ–¥–∫–∞ –Ω–∏–∂–µ üëá"
            tg_send_photo(lead_image, short_caption)
            tg_send_message(text)
    else:
        tg_send_message(text)

    state["seen_links"] = (state.get("seen_links", []) + new_links)[-2000:]
    save_json(STATE_FILE, state)


if __name__ == "__main__":
    main()
