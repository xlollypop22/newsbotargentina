import os, json, time, re
import feedparser
import requests
from dateutil import parser as dtparser
from openai import OpenAI

TELEGRAM_BOT_TOKEN = os.environ["TELEGRAM_BOT_TOKEN"]
TELEGRAM_CHANNEL = os.environ["TELEGRAM_CHANNEL"]

GROQ_API_KEY = os.environ.get("GROQ_API_KEY", "").strip()
if not GROQ_API_KEY:
    raise RuntimeError("GROQ_API_KEY is empty. Check GitHub Secrets.")

# Groq ‚Äî OpenAI-compatible endpoint
client = OpenAI(
    api_key=GROQ_API_KEY,
    base_url="https://api.groq.com/openai/v1",
)

TOTAL_LIMIT = int(os.environ.get("TOTAL_LIMIT", "5"))
PER_FEED_SCAN = int(os.environ.get("PER_FEED_SCAN", "10"))
MAX_SUMMARY_CHARS = int(os.environ.get("MAX_SUMMARY_CHARS", "280"))

FEEDS_FILE = "feeds.json"
STATE_FILE = "state.json"
from collections import defaultdict

# --- –†–£–ë–†–ò–ö–ò / –ì–û–†–Ø–ß–ï–ï ---

HOT_HOURS = int(os.environ.get("HOT_HOURS", "6"))  # —Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ —Å—á–∏—Ç–∞–µ–º "–≥–æ—Ä—è—á–∏–º"
MAX_PER_RUBRIC = int(os.environ.get("MAX_PER_RUBRIC", "2"))  # –º–∞–∫—Å–∏–º—É–º –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ä—É–±—Ä–∏–∫–µ –∑–∞ –ø–æ—Å—Ç
HOT_MAX = int(os.environ.get("HOT_MAX", "2"))  # –º–∞–∫—Å–∏–º—É–º "–≥–æ—Ä—è—á–∏—Ö" –∑–∞ –ø–æ—Å—Ç

RUBRICS = {
    # –°–ª—É–∂–µ–±–Ω–∞—è —Ä—É–±—Ä–∏–∫–∞: –ø–æ–ø–∞–¥–∞–Ω–∏–µ —Å—é–¥–∞ = –≥–æ—Ä—è—á–µ–µ
    "üî• –ì–æ—Ä—è—á–µ–µ": [
        "urgente", "√∫ltimo momento", "ultima hora", "en vivo", "ahora", "breaking",
        "alerta", "se confirm√≥", "confirm√≥", "confirmaron"
    ],

    "üèõ –ü–æ–ª–∏—Ç–∏–∫–∞": [
        "milei", "presidente", "gobierno", "gabinete", "casa rosada", "jefe de gabinete",
        "congreso", "senado", "diputados", "ley", "decreto", "dnu", "bolet√≠n oficial",
        "oposici√≥n", "peronismo", "kirchnerismo", "cambiemos", "pro", "ucr", "lilia lemoine",
        "kicillof", "massa", "bullrich", "macri", "larreta", "patricia bullrich",
        "elecciones", "balotaje", "campa√±a"
    ],

    "üí∞ –≠–∫–æ–Ω–æ–º–∏–∫–∞": [
        "econom√≠a", "inflaci√≥n", "inflacion", "ipc", "√≠ndice", "indec", "recesi√≥n", "recesion",
        "d√≥lar", "dolar", "blue", "mep", "ccl", "reservas", "banco central", "bcr", "bCRA",
        "fmi", "deuda", "bonos", "mercados", "riesgo pa√≠s", "riesgo pais", "tasas",
        "exportaciones", "importaciones", "subsidios", "tarifas", "salarios", "paritarias",
        "pymes", "impuestos", "retenciones", "cepo", "devaluaci√≥n", "devaluacion"
    ],

    "‚öñÔ∏è –°—É–¥ / –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å": [
        "polic√≠a", "policia", "crimen", "delito", "robo", "homicidio", "asesinato",
        "detenido", "detuvieron", "allanamiento", "operativo", "narco", "drogas",
        "juez", "jueza", "fiscal", "tribunal", "causa", "condena", "juicio",
        "seguridad", "gendarmer√≠a", "gendarmeria", "prefectura"
    ],

    "üåé –û–±—â–µ—Å—Ç–≤–æ": [
        "salud", "hospital", "educaci√≥n", "educacion", "escuela", "universidad",
        "paro", "huelga", "sindicato", "cgt", "protesta", "marcha",
        "transporte", "subte", "colectivo", "tren", "aerolineas",
        "vivienda", "alquiler", "inmuebles", "corte", "piquete",
        "servicios", "luz", "gas", "agua", "seguro", "anmat"
    ],

    "üè¢ –ë–∏–∑–Ω–µ—Å / –∫–æ–º–ø–∞–Ω–∏–∏": [
        "empresa", "empresas", "negocio", "negocios", "inversi√≥n", "inversion",
        "startup", "fintech", "banco", "bancos", "mercado libre", "ypf",
        "telecom", "personal", "movistar", "claro", "aerol√≠neas", "aerolineas",
        "exportador", "importador", "industria", "comercio"
    ],

    "üß™ –ù–∞—É–∫–∞ / —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": [
        "tecnolog√≠a", "tecnologia", "ia", "inteligencia artificial", "software",
        "ciber", "ciberseguridad", "hack", "datos", "internet", "sat√©lite", "satelite",
        "investigaci√≥n", "investigacion", "conicet"
    ],

    "üå¶ –ü–æ–≥–æ–¥–∞ / –ß–°": [
        "tormenta", "lluvia", "granizo", "ola de calor", "ola de frio", "inundaci√≥n", "inundacion",
        "alerta meteorol√≥gica", "alerta meteorologica", "evacuados", "incendio", "sismo"
    ],

    "üé≠ –ö—É–ª—å—Ç—É—Ä–∞": [
        "cultura", "cine", "teatro", "m√∫sica", "musica", "festival", "libro", "feria del libro",
        "arte", "exposici√≥n", "exposicion", "concierto"
    ],

    "‚öΩ –°–ø–æ—Ä—Ç": [
        "f√∫tbol", "futbol", "river", "boca", "selecci√≥n", "seleccion", "messi",
        "copa", "liga", "mundial", "aFA", "racing", "independiente", "san lorenzo"
    ],
}

RUBRIC_ORDER = [
    "üî• –ì–æ—Ä—è—á–µ–µ",
    "üí∞ –≠–∫–æ–Ω–æ–º–∏–∫–∞",
    "üèõ –ü–æ–ª–∏—Ç–∏–∫–∞",
    "üè¢ –ë–∏–∑–Ω–µ—Å / –∫–æ–º–ø–∞–Ω–∏–∏",
    "‚öñÔ∏è –°—É–¥ / –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
    "üåé –û–±—â–µ—Å—Ç–≤–æ",
    "üß™ –ù–∞—É–∫–∞ / —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
    "üå¶ –ü–æ–≥–æ–¥–∞ / –ß–°",
    "üé≠ –ö—É–ª—å—Ç—É—Ä–∞",
    "‚öΩ –°–ø–æ—Ä—Ç",
]

# –ï—Å–ª–∏ —Ö–æ—á–µ—à—å —Å—Ç—Ä–æ–≥–æ "—Ç–æ–ª—å–∫–æ –ø—Ä–æ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É" ‚Äî –æ—Å—Ç–∞–≤—å –≤–∫–ª—é—á—ë–Ω–Ω—ã–º
ARG_FILTER = os.environ.get("ARG_FILTER", "1") == "1"
ARG_HINTS = [
    "argentina", "argentino", "buenos aires", "caba", "amba",
    "c√≥rdoba", "cordoba", "rosario", "mendoza", "la plata",
    "santa fe", "tucum√°n", "tucuman", "salta", "neuqu√©n", "neuquen",
    "milei", "casa rosada", "congreso", "banco central", "indec",
]

def is_argentina_related(title: str, summary: str, link: str = "") -> bool:
    if not ARG_FILTER:
        return True
    t = (title + " " + summary + " " + (link or "")).lower()
    return any(h in t for h in ARG_HINTS)

def is_hot(ts: float, title: str, summary: str) -> bool:
    if (time.time() - ts) <= HOT_HOURS * 3600:
        return True
    t = (title + " " + summary).lower()
    return any(w in t for w in RUBRICS["üî• –ì–æ—Ä—è—á–µ–µ"])

def detect_rubric(ts: float, title: str, summary: str) -> str:
    if is_hot(ts, title, summary):
        return "üî• –ì–æ—Ä—è—á–µ–µ"
    t = (title + " " + summary).lower()
    for rubric in RUBRIC_ORDER:
        if rubric == "üî• –ì–æ—Ä—è—á–µ–µ":
            continue
        keys = RUBRICS.get(rubric, [])
        if any(k in t for k in keys):
            return rubric
    return "üåé –û–±—â–µ—Å—Ç–≤–æ"

def load_json(path, default):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return default


def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def tg_send_message(text: str):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    r = requests.post(
        url,
        json={
            "chat_id": TELEGRAM_CHANNEL,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        },
        timeout=30,
    )
    r.raise_for_status()


def html_escape(s: str) -> str:
    return (s or "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")


def pick_time(entry) -> float:
    for key in ("published", "updated", "created"):
        if key in entry and entry[key]:
            try:
                return dtparser.parse(entry[key]).timestamp()
            except Exception:
                pass
    return time.time()


def clean_text(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s


def _call_groq_chat(messages, model: str, max_retries: int = 3):
    # –ø—Ä–æ—Å—Ç–æ–π —Ä–µ—Ç—Ä–∞–π –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏/–ª–∏–º–∏—Ç—ã
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.2,
            )
        except Exception as e:
            msg = str(e)
            # –≥—Ä—É–±–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: 429/5xx/timeout
            if attempt < max_retries - 1 and (
                "429" in msg or "Rate limit" in msg or "timeout" in msg or "5" in msg
            ):
                time.sleep(1.5 * (attempt + 1))
                continue
            raise


def summarize_to_ru(title: str, snippet: str) -> str:
    title = clean_text(title)
    snippet = clean_text(snippet)

    base = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n–¢–µ–∫—Å—Ç: {snippet}" if snippet else f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}"

    # ‚úÖ Groq –º–æ–¥–µ–ª–∏ (–≤—ã–±–µ—Ä–∏ –æ–¥–Ω—É):
    # - "llama-3.3-70b-versatile" (–ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –º–µ–¥–ª–µ–Ω–Ω–µ–µ/–¥–æ—Ä–æ–∂–µ)
    # - "llama-3.1-8b-instant"   (–±—ã—Å—Ç—Ä–µ–µ/–¥–µ—à–µ–≤–ª–µ)
    model = os.environ.get("GROQ_MODEL", "llama-3.1-8b-instant")

    resp = _call_groq_chat(
        model=model,
        messages=[
            {
                "role": "system",
                "content": (
                    "–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
                    "–°—Ç–∏–ª—å: –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–Ω—ã–π, –±–µ–∑ –æ—Ü–µ–Ω–∫–∏ –∏ –∫–ª–∏—à–µ. "
                    "–î–ª–∏–Ω–∞: 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –ù–µ –¥–æ–±–∞–≤–ª—è–π —Ñ–∞–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤–æ –≤—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ."
                ),
            },
            {"role": "user", "content": base},
        ],
    )

    text = (resp.choices[0].message.content or "").strip()
    text = clean_text(text)
    if len(text) > MAX_SUMMARY_CHARS:
        text = text[: MAX_SUMMARY_CHARS - 1].rstrip() + "‚Ä¶"
    return text


def main():
    feeds = load_json(FEEDS_FILE, [])
    state = load_json(STATE_FILE, {"seen_links": []})
    seen = set(state.get("seen_links", []))

    candidates = []

    for f in feeds:
        name, url = f["name"], f["url"]
        d = feedparser.parse(url)

        entries = []
        for e in d.entries[:PER_FEED_SCAN]:
            link = getattr(e, "link", None)
            title = getattr(e, "title", "").strip()
            summary = getattr(e, "summary", "") or getattr(e, "description", "") or ""

            if not link or not title:
                continue
            if link in seen:
                continue

            ts = pick_time(e)
            entries.append((ts, name, title, link, summary))

        entries.sort(key=lambda x: x[0], reverse=True)
        candidates.extend(entries)

    candidates.sort(key=lambda x: x[0], reverse=True)
    picked = candidates[:TOTAL_LIMIT]

    if not picked:
        tg_send_message("–°–µ–≥–æ–¥–Ω—è –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –Ω–µ –Ω–∞—à—ë–ª.")
        return
    # --- –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä—É–±—Ä–∏–∫–∞–º + –ª–∏–º–∏—Ç—ã –Ω–∞ —Ä—É–±—Ä–∏–∫—É ---
    grouped = defaultdict(list)

    for ts, source, title, link, summary in picked:
        if not is_argentina_related(title, summary, link):
            continue
        rubric = detect_rubric(ts, title, summary)
        grouped[rubric].append((ts, source, title, link, summary))

    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ ARG-—Ñ–∏–ª—å—Ç—Ä–∞ –Ω–∏—á–µ–≥–æ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å ‚Äî —Å–æ–æ–±—â–∞–µ–º
    if not any(grouped.values()):
        tg_send_message("–°–µ–≥–æ–¥–Ω—è –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –Ω–µ –Ω–∞—à—ë–ª –Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É.")
        return

    lines = ["<b>–ê—Ä–≥–µ–Ω—Ç–∏–Ω–∞ ‚Äî –µ–∂–µ–¥–Ω–µ–≤–Ω–∞—è –≤—ã–∂–∏–º–∫–∞</b>\n"]
    new_links = []

    # –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ª–∏–º–∏—Ç—ã –¥–ª—è "–≥–æ—Ä—è—á–∏—Ö" –∏ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
    hot_left = HOT_MAX

    for rubric in RUBRIC_ORDER:
        items = grouped.get(rubric, [])
        if not items:
            continue

        # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–∏ —Ä—É–±—Ä–∏–∫–∏ –ø–æ —Å–≤–µ–∂–µ—Å—Ç–∏
        items.sort(key=lambda x: x[0], reverse=True)

        if rubric == "üî• –ì–æ—Ä—è—á–µ–µ":
            items = items[:hot_left]
            hot_left -= len(items)
            if not items:
                continue
        else:
            items = items[:MAX_PER_RUBRIC]

        lines.append(f"<b>{html_escape(rubric)}</b>")

        for ts, source, title, link, summary in items:
            ru = summarize_to_ru(title, summary)
            lines.append(f"‚Ä¢ <a href=\"{html_escape(link)}\">{html_escape(title)}</a> <i>({html_escape(source)})</i>")
            if ru:
                lines.append(f"  {html_escape(ru)}")
            new_links.append(link)

        lines.append("")

    text = "\n".join(lines).strip()
    if len(text) > 3800:
        text = text[:3790] + "‚Ä¶"

    tg_send_message(text)

    state["seen_links"] = (state.get("seen_links", []) + new_links)[-2000:]
    save_json(STATE_FILE, state)

if __name__ == "__main__":
    main()
