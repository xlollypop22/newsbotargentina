import os, json, time, re
from collections import defaultdict
from typing import Optional, List, Tuple, Dict

import feedparser
import requests
from dateutil import parser as dtparser
from openai import OpenAI


# ----------------- ENV / CLIENT -----------------

TELEGRAM_BOT_TOKEN = os.environ["TELEGRAM_BOT_TOKEN"]
TELEGRAM_CHANNEL = os.environ["TELEGRAM_CHANNEL"]

GROQ_API_KEY = os.environ.get("GROQ_API_KEY", "").strip()
if not GROQ_API_KEY:
    raise RuntimeError("GROQ_API_KEY is empty. Check GitHub Secrets.")

client = OpenAI(
    api_key=GROQ_API_KEY,
    base_url="https://api.groq.com/openai/v1",
)

GROQ_MODEL = os.environ.get("GROQ_MODEL", "llama-3.1-8b-instant")

FEEDS_FILE = os.environ.get("FEEDS_FILE", "feeds.json")
STATE_FILE = os.environ.get("STATE_FILE", "state.json")

TOTAL_LIMIT = int(os.environ.get("TOTAL_LIMIT", "40"))        # –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤ (—á—É—Ç—å –±–æ–ª—å—à–µ, —á—Ç–æ–±—ã –¥–æ—Ç—è–Ω—É—Ç—å –¥–æ MIN_NEWS)
PER_FEED_SCAN = int(os.environ.get("PER_FEED_SCAN", "30"))    # –∑–∞–ø–∏—Å–µ–π –∏–∑ RSS –Ω–∞ —Ñ–∏–¥

HOT_HOURS = int(os.environ.get("HOT_HOURS", "6"))

ARG_FILTER = os.environ.get("ARG_FILTER", "1") == "1"         # 1 = —Å—Ç—Ä–æ–≥–æ –ê—Ä–≥–µ–Ω—Ç–∏–Ω–∞

HTTP_TIMEOUT = int(os.environ.get("HTTP_TIMEOUT", "18"))

# –û—Ç–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏ ‚Äú—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å‚Äù
MIN_NEWS = int(os.environ.get("MIN_NEWS", "3"))               # !!! –º–∏–Ω–∏–º—É–º 3 –Ω–æ–≤–æ—Å—Ç–∏ –≤ –ø–æ—Å—Ç–µ
MAX_NEWS = int(os.environ.get("MAX_NEWS", "5"))               # –º–∞–∫—Å–∏–º—É–º –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –ø–æ—Å—Ç–µ
MIN_PER_TARGET = int(os.environ.get("MIN_PER_TARGET", "1"))   # —Ö–æ—Ç–∏–º —Ö–æ—Ç—è –±—ã –ø–æ 1 –∏–∑ –∫–∞–∂–¥–æ–π —Ü–µ–ª–µ–≤–æ–π —Ä—É–±—Ä–∏–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)

# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è Telegram caption (–¥–ª—è —Ñ–æ—Ç–æ) ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–µ—Ä–∂–∞—Ç—å < 900‚Äì950
CAPTION_LIMIT = int(os.environ.get("CAPTION_LIMIT", "950"))

# –ö–æ—Ä–æ—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞, —á—Ç–æ–±—ã –ø–æ–º–µ—â–∞—Ç—å—Å—è –≤ caption
MAX_SUMMARY_CHARS = int(os.environ.get("MAX_SUMMARY_CHARS", "170"))  # 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ–º–ø–∞–∫—Ç–Ω–æ


# ----------------- RUBRICS -----------------

RUBRICS = {
    "üî• –ì–æ—Ä—è—á–µ–µ": [
        "urgente", "√∫ltimo momento", "ultima hora", "en vivo", "ahora", "breaking",
        "alerta", "se confirm√≥", "confirm√≥", "confirmaron"
    ],
    "üí∞ –≠–∫–æ–Ω–æ–º–∏–∫–∞": [
        "econom√≠a", "economia", "inflaci√≥n", "inflacion", "ipc", "√≠ndice", "indice", "indec",
        "recesi√≥n", "recesion", "d√≥lar", "dolar", "blue", "mep", "ccl", "reservas",
        "banco central", "bcr", "bcra", "fmi", "deuda", "bonos", "mercados",
        "riesgo pa√≠s", "riesgo pais", "tasas", "exportaciones", "importaciones",
        "subsidios", "tarifas", "salarios", "paritarias", "pymes", "impuestos",
        "retenciones", "cepo", "devaluaci√≥n", "devaluacion"
    ],
    "üèõ –ü–æ–ª–∏—Ç–∏–∫–∞": [
        "milei", "presidente", "gobierno", "gabinete", "casa rosada", "jefe de gabinete",
        "congreso", "senado", "diputados", "ley", "decreto", "dnu", "bolet√≠n oficial", "boletin oficial",
        "oposici√≥n", "oposicion", "peronismo", "kirchnerismo", "pro", "ucr",
        "kicillof", "massa", "bullrich", "macri", "larreta", "elecciones", "balotaje", "campa√±a"
    ],
    "üè¢ –ë–∏–∑–Ω–µ—Å": [
        "empresa", "empresas", "negocio", "negocios", "inversi√≥n", "inversion",
        "startup", "fintech", "banco", "bancos", "mercado libre", "ypf",
        "telecom", "personal", "movistar", "claro", "aerol√≠neas", "aerolineas",
        "industria", "comercio", "inmobiliaria", "energ√≠a", "energia"
    ],
    "üé≠ –ö—É–ª—å—Ç—É—Ä–∞": [
        "cultura", "cine", "teatro", "m√∫sica", "musica", "festival", "libro",
        "feria del libro", "arte", "exposici√≥n", "exposicion", "concierto", "museo"
    ],
    "‚öΩ –°–ø–æ—Ä—Ç": [
        "f√∫tbol", "futbol", "river", "boca", "selecci√≥n", "seleccion", "messi",
        "copa", "liga", "mundial", "afa", "racing", "independiente", "san lorenzo",
        "tenis", "nba", "f1", "gran premio"
    ],
    "üåé –û–±—â–µ—Å—Ç–≤–æ": [
        "salud", "hospital", "educaci√≥n", "educacion", "escuela", "universidad",
        "paro", "huelga", "sindicato", "cgt", "protesta", "marcha",
        "transporte", "subte", "colectivo", "tren", "vivienda", "alquiler",
        "servicios", "luz", "gas", "agua", "anmat"
    ],
}

# –¶–ï–õ–ï–í–´–ï —Ä—É–±—Ä–∏–∫–∏ (–ø–æ—Ä—è–¥–æ–∫ –≤ –ø–æ—Å—Ç–µ)
TARGET_RUBRICS = ["üèõ –ü–æ–ª–∏—Ç–∏–∫–∞", "üí∞ –≠–∫–æ–Ω–æ–º–∏–∫–∞", "üè¢ –ë–∏–∑–Ω–µ—Å", "üé≠ –ö—É–ª—å—Ç—É—Ä–∞", "‚öΩ –°–ø–æ—Ä—Ç"]

# –ê—Ä–≥–µ–Ω—Ç–∏–Ω—Å–∫–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ (—Ç–µ–∫—Å—Ç + URL)
ARG_HINTS = [
    "argentina", "argentino", "buenos aires", "caba", "amba", "gba",
    "c√≥rdoba", "cordoba", "rosario", "mendoza", "la plata",
    "santa fe", "tucum√°n", "tucuman", "salta", "neuqu√©n", "neuquen",
    "san juan", "san luis", "chaco", "misiones", "corrientes",
    "entre r√≠os", "entre rios", "r√≠o negro", "rio negro",
    "chubut", "santa cruz", "tierra del fuego", "ushuaia",
    "mar del plata", "bah√≠a blanca", "bahia blanca",
    "milei", "casa rosada", "gobierno", "presidente",
    "congreso", "senado", "diputados", "bolet√≠n oficial", "boletin oficial",
    "indec", "banco central", "bcra", "afip", "anmat",
    "subte", "colectivo", "tren roca", "tren mitre", "tren sarmiento",
    "aerolineas argentinas", "ypf", "mercado libre", "edenor", "edesur",
]

# –Ø–≤–Ω—ã–µ ‚Äú–∞—Ä–≥–µ–Ω—Ç–∏–Ω—Å–∫–∏–µ‚Äù –º–∞—Ä–∫–µ—Ä—ã –≤ URL (—Å–∏–ª—å–Ω–µ–µ –¥–æ–º–µ–Ω–∞)
ARG_URL_MARKERS = [
    "/argentina", "/buenos-aires", "/caba", "/amba",
    "/cordoba", "/rosario", "/mendoza", "/la-plata",
    "/santa-fe", "/tucuman", "/salta", "/neuquen",
    "/san-juan", "/san-luis", "/chaco", "/misiones", "/corrientes",
    "/entre-rios", "/rio-negro", "/chubut", "/santa-cruz", "/tierra-del-fuego",
    "/mar-del-plata", "/bahia-blanca", "/ushuaia",
]


# ----------------- JSON / TELEGRAM -----------------

def load_json(path, default):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return default


def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def tg_send_message(text: str):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    r = requests.post(
        url,
        json={
            "chat_id": TELEGRAM_CHANNEL,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        },
        timeout=30,
    )
    r.raise_for_status()


def tg_send_photo(photo_url: str, caption: str):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendPhoto"
    r = requests.post(
        url,
        data={
            "chat_id": TELEGRAM_CHANNEL,
            "photo": photo_url,
            "caption": caption,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        },
        timeout=30,
    )
    r.raise_for_status()


# ----------------- HELPERS -----------------

def html_escape(s: str) -> str:
    return (s or "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")


def clean_text(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s


def pick_time(entry) -> float:
    for key in ("published", "updated", "created"):
        if key in entry and entry[key]:
            try:
                return dtparser.parse(entry[key]).timestamp()
            except Exception:
                pass
    return time.time()


def is_argentina_related(title: str, summary: str, link: str) -> bool:
    """
    –°—Ç—Ä–æ–≥–∏–π —Ñ–∏–ª—å—Ç—Ä "–ø—Ä–æ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É".

    –í–ê–ñ–ù–û: –¥–æ–º–µ–Ω –∞—Ä–≥–µ–Ω—Ç–∏–Ω—Å–∫–æ–≥–æ –°–ú–ò –ù–ï –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –Ω–æ–≤–æ—Å—Ç—å –ø—Ä–æ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É,
    –ø–æ—ç—Ç–æ–º—É –Ω–∏–∫–∞–∫–∏—Ö –∞–≤—Ç–æ-True –ø–æ –¥–æ–º–µ–Ω—É.
    """
    if not ARG_FILTER:
        return True

    title = title or ""
    summary = summary or ""
    link = link or ""

    blob = (title + " " + summary + " " + link).lower()

    # 1) –°–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª: –∞—Ä–≥–µ–Ω—Ç–∏–Ω—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã –≤ —Ç–µ–∫—Å—Ç–µ/URL
    if any(h in blob for h in ARG_HINTS):
        return True

    # 2) –°–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª: —è–≤–Ω–∞—è –≥–µ–æ–≥—Ä–∞—Ñ–∏—è/—Å–µ–∫—Ü–∏—è –≤ URL
    url_l = link.lower()
    if any(m in url_l for m in ARG_URL_MARKERS):
        return True

    return False


def is_hot(ts: float, title: str, summary: str) -> bool:
    if (time.time() - ts) <= HOT_HOURS * 3600:
        return True
    t = ((title or "") + " " + (summary or "")).lower()
    return any(w in t for w in RUBRICS["üî• –ì–æ—Ä—è—á–µ–µ"])


def detect_rubric(ts: float, title: str, summary: str) -> str:
    t = ((title or "") + " " + (summary or "")).lower()

    for rubric in TARGET_RUBRICS:
        if any(k in t for k in RUBRICS.get(rubric, [])):
            return rubric

    return "üåé –û–±—â–µ—Å—Ç–≤–æ"


# ----------------- IMAGE EXTRACTION (RSS -> HTML og:image) -----------------

UA = "Mozilla/5.0 (compatible; ArgentinaDigestBot/1.2; +https://github.com/)"

def extract_image_from_rss(entry) -> Optional[str]:
    if hasattr(entry, "media_content"):
        try:
            for m in entry.media_content:
                u = m.get("url")
                if u:
                    return u
        except Exception:
            pass

    if hasattr(entry, "links"):
        try:
            for l in entry.links:
                href = l.get("href")
                ltype = (l.get("type") or "").lower()
                rel = (l.get("rel") or "").lower()
                if href and (ltype.startswith("image/") or rel == "enclosure"):
                    return href
        except Exception:
            pass

    summary = getattr(entry, "summary", "") or getattr(entry, "description", "") or ""
    m = re.search(r'<img[^>]+src="([^"]+)"', summary)
    if m:
        return m.group(1)

    return None


def extract_og_image_from_html(url: str) -> Optional[str]:
    try:
        r = requests.get(
            url,
            headers={"User-Agent": UA},
            timeout=HTTP_TIMEOUT,
            allow_redirects=True,
        )
        if r.status_code >= 400:
            return None
        html = r.text
    except Exception:
        return None

    patterns = [
        r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']',
        r'<meta[^>]+content=["\']([^"\']+)["\'][^>]+property=["\']og:image["\']',
        r'<meta[^>]+name=["\']twitter:image["\'][^>]+content=["\']([^"\']+)["\']',
        r'<meta[^>]+content=["\']([^"\']+)["\'][^>]+name=["\']twitter:image["\']',
    ]
    for p in patterns:
        m = re.search(p, html, flags=re.IGNORECASE)
        if m:
            img = m.group(1).strip()
            if img.startswith("//"):
                img = "https:" + img
            return img

    return None


def best_image(entry, link: str) -> Optional[str]:
    img = extract_image_from_rss(entry)
    if img:
        return img
    return extract_og_image_from_html(link)


# ----------------- GROQ SUMMARIZER -----------------

def _call_groq_chat(messages, model: str, max_retries: int = 3):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.2,
                max_tokens=200,
            )
        except Exception as e:
            msg = str(e)
            if attempt < max_retries - 1 and (
                "429" in msg or "Rate limit" in msg or "timeout" in msg or "5" in msg
            ):
                time.sleep(1.5 * (attempt + 1))
                continue
            raise


def summarize_to_ru(title: str, snippet: str) -> str:
    title = clean_text(title)
    snippet = clean_text(snippet)

    base = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n–¢–µ–∫—Å—Ç: {snippet}" if snippet else f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}"

    resp = _call_groq_chat(
        model=GROQ_MODEL,
        messages=[
            {
                "role": "system",
                "content": (
                    "–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –∏ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π. –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –Ω–∞ –∏—Å–ø–∞–Ω—Å–∫–æ–º (–ê—Ä–≥–µ–Ω—Ç–∏–Ω–∞). "
                    "–°–¥–µ–ª–∞–π –≤—ã–∂–∏–º–∫—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º: 1‚Äì2 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ –∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–Ω–æ, –±–µ–∑ –æ—Ü–µ–Ω–∫–∏ –∏ –∫–ª–∏—à–µ. "
                    "–ù–µ –¥–æ–±–∞–≤–ª—è–π —Ñ–∞–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ."
                ),
            },
            {"role": "user", "content": base},
        ],
    )

    text = clean_text((resp.choices[0].message.content or "").strip())
    if len(text) > MAX_SUMMARY_CHARS:
        text = text[: MAX_SUMMARY_CHARS - 1].rstrip() + "‚Ä¶"
    return text


# ----------------- MAIN -----------------

Item = Tuple[float, str, str, str, str, Optional[str]]  # (ts, source, title, link, summary, image_url)

def score_item(ts: float, title: str, summary: str) -> int:
    s = 0
    if is_hot(ts, title, summary):
        s += 3
    age_hours = max(0.0, (time.time() - ts) / 3600.0)
    if age_hours <= 6:
        s += 3
    elif age_hours <= 24:
        s += 2
    elif age_hours <= 72:
        s += 1
    return s


def build_single_caption(selected: List[Tuple[str, Item]]) -> str:
    lines: List[str] = ["<b>–ê—Ä–≥–µ–Ω—Ç–∏–Ω–∞ ‚Äî –¥–∞–π–¥–∂–µ—Å—Ç</b>"]
    current = None

    for rubric, (ts, source, title, link, summary, image_url) in selected:
        if rubric != current:
            lines.append("")
            lines.append(f"<b>{html_escape(rubric)}</b>")
            current = rubric

        ru = summarize_to_ru(title, summary)

        lines.append(f"‚Ä¢ <a href=\"{html_escape(link)}\">{html_escape(clean_text(title))}</a>")
        if ru:
            lines.append(f"  {html_escape(ru)}")
        lines.append("")

        if len("\n".join(lines)) > CAPTION_LIMIT:
            while lines and len("\n".join(lines)) > (CAPTION_LIMIT - 10):
                lines.pop()
            lines.append("‚Ä¶")
            break

    text = "\n".join(lines).strip()
    if len(text) > CAPTION_LIMIT:
        text = text[: CAPTION_LIMIT - 1].rstrip() + "‚Ä¶"
    return text


def main():
    feeds = load_json(FEEDS_FILE, [])
    state = load_json(STATE_FILE, {"seen_links": []})
    seen = set(state.get("seen_links", []))

    candidates: List[Item] = []

    for f in feeds:
        name, url = f["name"], f["url"]
        d = feedparser.parse(url)

        entries: List[Item] = []
        for e in d.entries[:PER_FEED_SCAN]:
            link = getattr(e, "link", None)
            title = getattr(e, "title", "").strip()
            summary = getattr(e, "summary", "") or getattr(e, "description", "") or ""

            if not link or not title:
                continue
            if link in seen:
                continue

            ts = pick_time(e)
            image_url = best_image(e, link)

            entries.append((ts, name, title, link, summary, image_url))

        entries.sort(key=lambda x: x[0], reverse=True)
        candidates.extend(entries)

    candidates.sort(key=lambda x: x[0], reverse=True)
    candidates = candidates[:TOTAL_LIMIT]

    # —Ñ–∏–ª—å—Ç—Ä –ê—Ä–≥–µ–Ω—Ç–∏–Ω—ã (—Å—Ç—Ä–æ–≥–∏–π)
    filtered: List[Item] = []
    for it in candidates:
        ts, source, title, link, summary, image_url = it
        if is_argentina_related(title, summary, link):
            filtered.append(it)

    if not filtered:
        tg_send_message("–°–µ–≥–æ–¥–Ω—è –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –Ω–µ –Ω–∞—à—ë–ª –Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É.")
        return

    # –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä—É–±—Ä–∏–∫–∞–º + —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≤–Ω—É—Ç—Ä–∏ —Ä—É–±—Ä–∏–∫ –ø–æ (score, ts)
    grouped: Dict[str, List[Item]] = defaultdict(list)
    for it in filtered:
        ts, source, title, link, summary, image_url = it
        r = detect_rubric(ts, title, summary)
        grouped[r].append(it)

    for r, items in grouped.items():
        items.sort(key=lambda x: (score_item(x[0], x[2], x[4]), x[0]), reverse=True)

    # -------- –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–´–ô –û–¢–ë–û–† --------
    selected: List[Tuple[str, Item]] = []

    # 1) –º–∏–Ω–∏–º—É–º –ø–æ 1 –∏–∑ –∫–∞–∂–¥–æ–π —Ü–µ–ª–µ–≤–æ–π —Ä—É–±—Ä–∏–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    for r in TARGET_RUBRICS:
        items = grouped.get(r, [])
        take = min(MIN_PER_TARGET, len(items))
        for i in range(take):
            selected.append((r, items[i]))

    # 2) –¥–æ–±–∏–≤–∞–µ–º –¥–æ MAX_NEWS –ª—É—á—à–∏–º–∏ –æ—Å—Ç–∞–≤—à–∏–º–∏—Å—è –∏–∑ —Ü–µ–ª–µ–≤—ã—Ö —Ä—É–±—Ä–∏–∫
    used_links = {it[3] for _, it in selected}
    if len(selected) < MAX_NEWS:
        pool: List[Tuple[str, Item]] = []
        for r in TARGET_RUBRICS:
            for it in grouped.get(r, []):
                if it[3] not in used_links:
                    pool.append((r, it))

        pool.sort(key=lambda x: (score_item(x[1][0], x[1][2], x[1][4]), x[1][0]), reverse=True)

        for r, it in pool:
            if len(selected) >= MAX_NEWS:
                break
            selected.append((r, it))
            used_links.add(it[3])

    # 3) –µ—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –º–∞–ª–æ ‚Äî –¥–æ–±–∏—Ä–∞–µ–º –∏–∑ –ª—é–±—ã—Ö —Ä—É–±—Ä–∏–∫ (–≤–∫–ª—é—á–∞—è ‚Äú–û–±—â–µ—Å—Ç–≤–æ‚Äù)
    if len(selected) < MIN_NEWS:
        pool2: List[Tuple[str, Item]] = []
        for r, items in grouped.items():
            for it in items:
                if it[3] not in used_links:
                    pool2.append((r, it))

        pool2.sort(key=lambda x: (score_item(x[1][0], x[1][2], x[1][4]), x[1][0]), reverse=True)

        for r, it in pool2:
            if len(selected) >= MIN_NEWS:
                break
            selected.append((r, it))
            used_links.add(it[3])

    # –µ—Å–ª–∏ –ø–æ—Å–ª–µ –¥–æ–±–æ—Ä–∞ –≤—Å—ë —Ä–∞–≤–Ω–æ –ø—É—Å—Ç–æ (–∫—Ä–∞–π–Ω–µ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ)
    if not selected:
        tg_send_message("–°–µ–≥–æ–¥–Ω—è –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –ø–æ–¥–±–æ—Ä–∫—É.")
        return

    # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –º–∏–Ω–∏–º—É–º 3 (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ) –∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ–º MAX_NEWS
    if len(selected) < MIN_NEWS:
        tg_send_message("–°–µ–≥–æ–¥–Ω—è —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º.")
        return

    if len(selected) > MAX_NEWS:
        selected = selected[:MAX_NEWS]

    # –ø–æ—Ä—è–¥–æ–∫ –≤ –ø–æ—Å—Ç–µ: —Ü–µ–ª–µ–≤—ã–µ —Ä—É–±—Ä–∏–∫–∏ –ø–µ—Ä–≤—ã–º–∏
    order_index = {r: i for i, r in enumerate(TARGET_RUBRICS)}
    selected.sort(key=lambda x: (order_index.get(x[0], 999), -x[1][0]))

    # -------- –û–î–ò–ù –ü–û–°–¢ –í TG: –§–û–¢–û + CAPTION --------
    # –±–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É —Å—Ä–µ–¥–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö
    lead_image = None
    for _, it in selected:
        if it[5]:
            lead_image = it[5]
            break

    caption = build_single_caption(selected)

    if lead_image:
        tg_send_photo(lead_image, caption)
    else:
        # –µ—Å–ª–∏ –Ω–∏ —É –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ ‚Äî –æ—Ç–ø—Ä–∞–≤–∏–º —Ç–µ–∫—Å—Ç–æ–º (–∏–Ω–∞—á–µ –Ω–∏–∫–∞–∫)
        tg_send_message(caption)

    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º seen
    new_links = [it[3] for _, it in selected]
    state["seen_links"] = (state.get("seen_links", []) + new_links)[-2500:]
    save_json(STATE_FILE, state)


if __name__ == "__main__":
    main()
